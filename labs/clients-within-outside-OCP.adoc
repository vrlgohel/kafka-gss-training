**Connecting from outside OCP**

* Configuring external routes

In certain scenarios, a Kafka cluster deployed in OpenShift may need to be accessed from outside the cluster.

First, we need to reconfigure the cluster with an `external` listener.

````
apiVersion: kafka.strimzi.io/v1alpha1
kind: Kafka
metadata:
  name: production-ready
spec:
  kafka:
    replicas: 3
    listeners:
      plain: {}
      tls: {}
      external:
        type: route
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: persistent-claim
      size: 3Gi
      deleteClaim: false
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}
````

Now let's apply this configuration to the running cluster.

````
oc apply -f /configurations/clusters/production-ready-external-routes.yaml
````

You can see that the Cluster Operator is starting a rolling update on the Kafka cluster, restarting each broker one by one for updating their configuration in order to expose the cluster outside of OpenShift.
Now you can inspect the existing services.
Notice that each of the brokers in the cluster has a route and a new bootstrap service has been created for external connections.

````
oc get routes
````

* Connecting external clients

For interacting with the broker, external clients must use TLS.
First, we need to extract the certificate of the server.

````
oc extract secret/production-ready-cluster-ca-cert --keys=ca.crt --to=- >certificate.crt
````

Then, we need to install it into a Java keystore.

````
keytool -import -trustcacerts -alias root -file certificate.crt -keystore keystore.jks -storepass password -noprompt
````

We can now run our producer and consumer applications by using this certificate.

````
wget -O log-consumer.jar?raw=true
wget -O timer-producer.jar?raw=true 
````

Launch the two applications with new configuration settings (replace <GUID> with your workstation GUID):

````
java -jar log-consumer.jar \
      --camel.component.kafka.configuration.brokers=production-ready-kafka-bootstrap-amq-streams.apps-<GUID>:443 \
      --camel.component.kafka.configuration.security-protocol=SSL \
      --camel.component.kafka.configuration.ssl-truststore-location=keystore.jks \
      --camel.component.kafka.configuration.ssl-truststore-password=password
````

````
java -jar timer-producer.jar \
      --camel.component.kafka.configuration.brokers=production-ready-kafka-bootstrap-amq-streams.apps-<GUID>:443 \
      --camel.component.kafka.configuration.security-protocol=SSL \
      --camel.component.kafka.configuration.ssl-truststore-location=keystore.jks \
      --camel.component.kafka.configuration.ssl-truststore-password=password --server.port=0
````

After observing how the two clients are exchaning messages from outside the OpenShift cluster, exit both the applications via `CTRL-C`.